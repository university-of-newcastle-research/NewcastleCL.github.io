---
title: "Why does t0 estimate well in PMwG?"
author: "Reilly Innes"
date: "21/01/2021"
output: html_document
---

```{r setup, include=FALSE}
library(knitr)
library(rtdists)
library(dplyr)
library(ggplot2)
library(tidyr)
library(ggpubr)
knitr::opts_chunk$set(echo = TRUE)
```

There has been substantial research and debate over the non-decision time parameter of evidence accumulation models. For LBA and diffusion models, there is often a discrepancy, where diffusion models show longer t0 times, with the LBA known to poorly estimate this parameter. Researchers are aware of this, however, the issue is largely ignored. Research into this parameter has used neural measurement data and muscular activity to provide useful estimates of t0 (i.e. the upper and lower limits of the parameter) to give a sense of how accurate this estimation is. 

Recently, following a paper by Gunawan et al., (2020), a new method of Bayesian hierarchical estimation was outlined which shows more reliable parameter estimation. Following several modelling exercises using this method (known as Particle Metropolis within Gibbs - PMwG), it was found that t0 estimates were much improved from previous DE-MCMC sampling methods. "Improved" being that estimates of t0 often centered around 0.01 seconds, whereas in PMwG, this was closer to 0.1 seconds - much more reasonable and in line with literature. So maybe it wasn't so much a problem with the model, but rather a problem with the model estimation. In this blog post, i explore why this might be. 




## First, parameter recovery

First of all, I'll make sure this holds in a parameter recovery exercise - after all, we would like to know if the LBA is still doing a not-so-great job of figuring out t0. In this simulation exercise, I show t0 recovering at 3 different values for a single t0 LBA model (i.e. one t0 for all simulated conditions) and 3 lots of t0 values for a three t0 model (i.e. each condition has a t0 value). In the code below, small, medium and large refer to the input t0 parameters (small = fast t0, large = slow t0).


### Single t0 

```{r singlet0recover, echo=FALSE}

load("~/Documents/Research/Modelling Project/Work/Projects/t0_samplers/recovery_t0samples.-1.6094379124341.Rdata")
big <- sampled
bigt0<-exp(ptm[7])
bigt0al <- exp(subj_random_effects[7,])
bigTheta <- exp(mean(big$samples$theta_mu[7,]))
bigAlpha <- exp(apply(big$samples$alpha[7,,],1,mean))

tmp<-c("Actual" = bigt0, "Recovered" = bigTheta)
kable(tmp, caption = "Large t0", col.names = c("t0"))



load("~/Documents/Research/Modelling Project/Work/Projects/t0_samplers/recovery_t0samples.-2.30258509299405.Rdata")
med<-sampled
medt0<-exp(ptm[7])
medt0al <- exp(subj_random_effects[7,])
medTheta <- exp(mean(med$samples$theta_mu[7,]))
medAlpha <- exp(apply(med$samples$alpha[7,,],1,mean))

tmp<-c("Actual" = medt0, "Recovered" = medTheta)
kable(tmp, caption = "Medium t0", col.names = c("t0"))


load("~/Documents/Research/Modelling Project/Work/Projects/t0_samplers/recovery_t0samples.-3.50655789731998.Rdata")
smal<-sampled
smalt0<-exp(ptm[7])
smalt0al <- exp(subj_random_effects[7,])
smalTheta <- exp(mean(smal$samples$theta_mu[7,]))
smalAlpha <- exp(apply(smal$samples$alpha[7,,],1,mean))
tmp<-c("Actual" = smalt0, "Recovered" = smalTheta)
kable(tmp, caption = "Small t0", col.names = c( "t0"))


```



#### Large
```{r alphasB, echo=FALSE, message=FALSE}



tmp<-as.data.frame(cbind(bigt0al,bigAlpha))
names(tmp)<- c("Actual", "Recovered")
tmp$subject <- c(1:20)
#kable(tmp[,c(3,1,2)], caption = "Big values")

tmp <- tmp %>% pivot_longer( cols = c("Actual","Recovered"), names_to = "Value", values_to = "t0")
tmp$Value<-as.factor(tmp$Value)
ggplot(tmp, aes(x=subject, y=t0, colour=Value))+geom_point()+geom_line()+ theme_bw()+stat_smooth()

```

#### Medium
```{r alphasM, echo=FALSE, message=FALSE}



tmp<-as.data.frame(cbind(medt0al,medAlpha))
names(tmp)<- c("Actual", "Recovered")
tmp$subject <- c(1:20)
#kable(tmp[,c(3,1,2)], caption = "Medium values")

tmp <- tmp %>% pivot_longer( cols = c("Actual","Recovered"), names_to = "Value", values_to = "t0")
tmp$Value<-as.factor(tmp$Value)
ggplot(tmp, aes(x=subject, y=t0, colour=Value))+geom_point()+geom_line()+ theme_bw()+stat_smooth()

```


```{r alphasS, echo=FALSE, message=FALSE}



tmp<-as.data.frame(cbind(smalt0al,smalAlpha))
names(tmp)<- c("Actual", "Recovered")
tmp$subject <- c(1:20)
#kable(tmp[,c(3,1,2)], caption = "Small values")

tmp <- tmp %>% pivot_longer( cols = c("Actual","Recovered"), names_to = "Value", values_to = "t0")
tmp$Value<-as.factor(tmp$Value)
ggplot(tmp, aes(x=subject, y=t0, colour=Value))+geom_point()+geom_line()+ theme_bw()+stat_smooth()

```

###  t0 varies with conds

```{r doubltt0recover, echo=FALSE}

load("~/Documents/Research/Modelling Project/Work/Projects/t0_samplers/recovery_t02_samples.large.Rdata")
big <- sampled
bigt0<-exp(ptm[7:9])
bigt0al <- exp(subj_random_effects[7:9,])
bigTheta <- exp(apply(big$samples$theta_mu[7:9,],1,mean))
bigAlpha <- exp(apply(big$samples$alpha[7:9,,],1:2,mean))

tmp<-c("Actual" = bigt0, "Recovered" = bigTheta)
kable(tmp, caption = "Large t0", col.names = c( "t0"))



load("~/Documents/Research/Modelling Project/Work/Projects/t0_samplers/recovery_t02_samples.med.Rdata")
med<-sampled
medt0<-exp(ptm[7:9])
medt0al <- exp(subj_random_effects[7:9,])
medTheta <- exp(apply(med$samples$theta_mu[7:9,],1,mean))
medAlpha <- exp(apply(med$samples$alpha[7:9,,],2,mean))

tmp<-c("Actual" = medt0, "Recovered" = medTheta)
kable(tmp, caption = "Medium t0", col.names = c( "t0"))


load("~/Documents/Research/Modelling Project/Work/Projects/t0_samplers/recovery_t02_samples.small.Rdata")
smal<-sampled
smalt0<-exp(ptm[7:9])
smalt0al <- exp(subj_random_effects[7:9,])
smalTheta <- exp(apply(smal$samples$theta_mu[7:9,],1,mean))
smalAlpha <- exp(apply(smal$samples$alpha[7:9,,],1:2,mean))
tmp<-c("Actual" = smalt0, "Recovered" = smalTheta)
kable(tmp, caption = "Small t0", col.names = c( "t0"))


```









So, it looks like t0 recovers relatively well, but maybe overestimates smaller values. This means that the LBA may still not perfectly estimate actual t0 values, but could also come from the variance in the individual subject synthetic parameters. One thing is for sure though, t0 is recovered at reasonable values compared to old DE-MCMC. So what could help this estimation method?


<!-- WHY?????? espeically for small values, it actually is bigger???? Check other params -->

## Log Transform

One answer to this question is the log transformation of the parameter vector. This was proposed in Gunawan et al., (2020) so that values drawn from PMwG, which are on the real number line, could be used with the LBA - which requires positive-definite values. Hence, when using the LBA in PMwG, we take the exponent of the proposal parameters to calculate the likelihood - where we return the log of the likelihood. 

Essentially by doing the log transform of the parameters, all particle values are sampled from the real number line. This makes it starightforward to specify their joint distribution as a multivariate normal (with full covariance matrix structure). Previously, we would've assumed that the prior joint distribution of the parameters was an uncorrelated, truncated (at zero) univariate normal. In this new approach, with the log transform (which in practice is just taking the exponential of the values on the real line), there are two key advantages; increasing sampling efficiency (sampling from the real line) and using more informed prior knowledge. further, previous methods could lead to overconfidence in the precision of estimation, and underestimation of the magnitide of individual differences - which could be key in estimating t0. With the covariance matrix able to be estimated, this links to the next section, but also, the log transform is necessary to better estimate this structure. 

To test the log transform, I use the Forstmann et al., (2008) dataset reported in Gunawan et al., (2020) for a 3 threshold LBA. Using the PMwG sampler, I fit the model twice, using varying likelihood functions - one which takes the exponent of the proposed values and one which returns bad likelihoods for proposed values below 0 - i.e. method one is the log transformed way, method two is the untransformed (but protected against negative values) way. 

The results are shown below. 


```{r thetaExpLogComparison, echo=FALSE}

load("~/Documents/Research/Modelling Project/Work/Projects/t0_samplers/forstmann08_sampled_exp.Rdata")
expd <- sampled
t0ex <- exp(mean(expd$samples$theta_mu[7,]))


load("~/Documents/Research/Modelling Project/Work/Projects/t0_samplers/forstmann08_sampled_log.Rdata")
logd <- sampled
t0lg <- mean(logd$samples$theta_mu[7,])

tmp<-c("Log Transformed" = t0ex, "Truncated values" = t0lg)
kable(tmp, caption = "t0 estimated values on the exponential scale (normal way) and on the real number line (logged)", col.names = c( "t0"))

```


### alpha values


```{r alphaExpLogComparison, echo=FALSE, message=FALSE}


t0exA <- exp(apply(expd$samples$alpha[7,,],1,mean))

t0lgA <- apply(logd$samples$alpha[7,,],1,mean)

tmp<-as.data.frame(cbind(t0exA,t0lgA))
names(tmp)<- c("log transformed", "truncated")
tmp$subject <- c(1:19)
#kable(tmp[,c(3,1,2)], caption = "t0 estimated values on the exponential scale (normal way) and on the real number line (logged)")

tmp <- tmp %>% pivot_longer( cols = c("truncated","log transformed"), names_to = "model", values_to = "t0")
tmp$model<-as.factor(tmp$model)
ggplot(tmp, aes(x=subject, y=t0, colour=model))+geom_point()+geom_line()+ theme_bw()+stat_smooth()
```



\pagebreak
## covariance matrix

In this section, I investigate whether the covariance matrix could be a main cause for better estimates of t0. As mentioned earlier, PMwG samples from the real line, making it easy to specify the parameters (particles) distribution as an unconstrained multivariate normal distribution with full covariance structure. Using the particle approach has an advantage over other MCMC methods as we can jointly estimate the density of parameters, which enables the covariance matrix to be informed, which then constrains proposed particles. 

To test whether the covariance matrix is a main cause for more accurate t0 estimation, I again ran the above model of the Forstmann et al., (2008) data set (with the log transform) over two different iterations. The first iteration ran PMwG as the R package defaults too. In the second iteration, I changed the v-half parameter, which is the hyperparameter on Σ prior (Half-t degrees of freedom). We tend to use v-half = 1. But to constrain the covariance matrix, I set v-half to 1000, essentially rendering the matrix useless. 

The following shows the results of this change. 

```{r vHalf, echo=FALSE, message=FALSE}
load("~/Documents/Research/Modelling Project/Work/Projects/t0_samplers/forstmann08_sampled_vHalf.Rdata")
vhalf<-adapted
vhalf_Theta <- exp(mean(vhalf$samples$theta_mu[7,]))
vhalf_alpha <- exp(apply(vhalf$samples$alpha[7,,],1,mean))

tmp<-c("Exponential with Covariance" = t0ex, "Without Covariance" = vhalf_Theta)
kable(tmp, caption = "t0 estimated with and without the covariance structure (using a high v-half)", col.names = c( "t0"))

``` 


### alpha values


```{r alphaVhalfComparison, echo=FALSE, message=FALSE}


t0v_half <- exp(apply(vhalf$samples$alpha[7,,],1,mean))

tmp<-as.data.frame(cbind(t0exA,t0v_half))
names(tmp)<- c("covariance", "non-covariance")
tmp$subject <- c(1:19)
#kable(tmp[,c(3,1,2)], caption = "t0 estimated values with v-half reasonable (allowing covariance matrix) and set to 1000 (no good covariance structure)")

tmp <- tmp %>% pivot_longer( cols = c("covariance","non-covariance"), names_to = "model", values_to = "t0")
tmp$model<-as.factor(tmp$model)
ggplot(tmp, aes(x=subject, y=t0, colour=model))+geom_point()+geom_line()+ theme_bw()+stat_smooth()
```


Evidently, removing the restraint from the covariance matrix leads to less reliable sampling and worse t0 estimates. Evidently, it is not directly one reason that t0 estimates more reliably in PMwG, but a combination of things. 


## a comment on hierarchical shrinkage

Evidently, although the sampler does a good job in recovering most values of t0, we still see some hierarchical shrinkage, especially with larger t0 values. This is to be expected with hierarchical Bayesian sampling models, however, should still be considered when reporting results. 

\pagebreak


## References

Forstmann, B. U., Dutilh, G., Brown, S., Neumann, J., Von Cramon, D. Y., Ridderinkhof, K. R., & Wagenmakers, E. J. (2008). Striatum and pre-SMA facilitate decision-making under time pressure. *Proceedings of the National Academy of Sciences, 105(45)*, 17538-17542.

Gunawan, D., Hawkins, G. E., Tran, M. N., Kohn, R., & Brown, S. D. (2020). New estimation approaches for the hierarchical Linear Ballistic Accumulator model. *Journal of Mathematical Psychology, 96*, 102368.



